// Copyright (c) 2025 WSO2 LLC (http://www.wso2.com).
//
// WSO2 LLC. licenses this file to you under the Apache License,
// Version 2.0 (the "License"); you may not use this file except
// in compliance with the License.
// You may obtain a copy of the License at
//
// http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing,
// software distributed under the License is distributed on an
// "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
// KIND, either express or implied.  See the License for the
// specific language governing permissions and limitations
// under the License.

import ballerina/ai;
import ballerina/http;
import ballerina/jballerina.java;

const DEFAULT_ANTHROPIC_SERVICE_URL = "https://api.anthropic.com/v1";
const DEFAULT_MAX_TOKEN_COUNT = 512;
const DEFAULT_TEMPERATURE = 0.7d;
const ANTHROPIC_API_VERSION = "2023-06-01";
const DEFAULT_GENERATOR_CONFIG = {};

# Provider is a client class that provides an interface for interacting with Anthropic Large Language Models.
public isolated client class ModelProvider {
    *ai:ModelProvider;
    private final http:Client AnthropicClient;
    private final string apiKey;
    private final string modelType;
    private final int maxTokens;
    private final decimal temperature;
    private final readonly & ai:GeneratorConfig generatorConfig;

    # Initializes the Anthropic model with the given connection configuration and model configuration.
    #
    # + apiKey - The Anthropic API key
    # + modelType - The Anthropic model name
    # + apiVersion - The Anthropic API version (e.g., "2023-06-01")  
    # + serviceUrl - The base URL of Anthropic API endpoint
    # + maxTokens - The upper limit for the number of tokens in the response generated by the model
    # + temperature - The temperature for controlling randomness in the model's output  
    # + connectionConfig - Additional HTTP connection configuration
    # + return - `nil` on successful initialization; otherwise, returns an `ai:Error`
    public isolated function init(string apiKey,
            @display {label: "Model Type"} ANTHROPIC_MODEL_NAMES modelType,
            @display {label: "Service URL"} string serviceUrl = DEFAULT_ANTHROPIC_SERVICE_URL,
            @display {label: "Maximum Tokens"} int maxTokens = DEFAULT_MAX_TOKEN_COUNT,
            @display {label: "Temperature"} decimal temperature = DEFAULT_TEMPERATURE,
            @display {label: "Generator Configuration"} 
            readonly & ai:GeneratorConfig generatorConfig = DEFAULT_GENERATOR_CONFIG,
            @display {label: "Connection Configuration"} *ConnectionConfig connectionConfig) returns ai:Error? {

        // Convert ConnectionConfig to http:ClientConfiguration
        http:ClientConfiguration anthropicConfig = {
            httpVersion: connectionConfig.httpVersion,
            http1Settings: connectionConfig.http1Settings ?: {},
            http2Settings: connectionConfig?.http2Settings ?: {},
            timeout: connectionConfig.timeout,
            forwarded: connectionConfig.forwarded,
            poolConfig: connectionConfig?.poolConfig,
            cache: connectionConfig?.cache ?: {},
            compression: connectionConfig.compression,
            circuitBreaker: connectionConfig?.circuitBreaker,
            retryConfig: connectionConfig?.retryConfig,
            responseLimits: connectionConfig?.responseLimits ?: {},
            secureSocket: connectionConfig?.secureSocket,
            proxy: connectionConfig?.proxy,
            validation: connectionConfig.validation
        };

        http:Client|error httpClient = new http:Client(serviceUrl, anthropicConfig);
        if httpClient is error {
            return error ai:Error("Failed to initialize Anthropic Model", httpClient);
        }

        self.AnthropicClient = httpClient;
        self.apiKey = apiKey;
        self.modelType = modelType;
        self.maxTokens = maxTokens;
        self.temperature = temperature;
        self.generatorConfig = generatorConfig;
    }

    # Uses Anthropic API to generate a response
    # + messages - List of chat messages or a user message
    # + tools - Tool definitions to be used for the tool call
    # + stop - Stop sequence to stop the completion (not used in this implementation)
    # + return - Chat response or an error in case of failures
    isolated remote function chat(ai:ChatMessage[]|ai:ChatUserMessage messages, ai:ChatCompletionFunctions[] tools = [], string? stop = ())
        returns ai:ChatAssistantMessage|ai:Error {

        // Map messages to Anthropic format
        AnthropicMessage[] anthropicMessages = check self.mapToAnthropicMessages(messages);

        // Prepare request payload
        map<json> requestPayload = {
            model: self.modelType,
            max_tokens: self.maxTokens,
            messages: anthropicMessages,
            temperature: self.temperature
        };

        if stop is string {
            requestPayload["stop_sequences"] = [stop];
        }

        // If tools are provided, add them to the request
        if tools.length() > 0 {
            requestPayload["tools"] = self.mapToAnthropicTools(tools);
        }

        // Send request to Anthropic API with proper headers
        map<string> headers = {
            "x-api-key": self.apiKey,
            "anthropic-version": ANTHROPIC_API_VERSION,
            "content-type": "application/json"
        };

        AnthropicApiResponse|error anthropicResponse = self.AnthropicClient->/messages.post(requestPayload, headers);
        if anthropicResponse is error {
            return error ai:LlmInvalidResponseError("Unexpected response format from Anthropic API", anthropicResponse);
        }

        string? content = ();
        ai:FunctionCall[] toolCalls = [];
        foreach ContentBlock block in anthropicResponse.content {
            string blockType = block.'type;
            if blockType == "tool_use" {
                toolCalls.push(check mapContentToFunctionCall(block));
            } else if blockType == "text" {
                content = block.text;
            }
        }
        return {role: ai:ASSISTANT, toolCalls: toolCalls == [] ? () : toolCalls, content};
    }

    # Sends a chat request to the model and generates a value that belongs to the type
    # corresponding to the type descriptor argument.
    # 
    # + prompt - The prompt to use in the chat messages
    # + td - Type descriptor specifying the expected return type format
    # + return - Generates a value that belongs to the type, or an error if generation fails
    isolated remote function generate(ai:Prompt prompt, @display {label: "Expected type"} typedesc<anydata> td = <>) 
                    returns td|ai:Error = @java:Method {
        'class: "io.ballerina.lib.ai.anthropic.Generator"
    } external;

    # Converts standard ai:ChatMessage array to Anthropic's message format
    #
    # + messages - List of chat messages or a user message
    # + return - return value description
    private isolated function mapToAnthropicMessages(ai:ChatMessage[]|ai:ChatUserMessage messages)
    returns AnthropicMessage[]|ai:Error {
        AnthropicMessage[] anthropicMessages = [];
        if messages is ai:ChatUserMessage {
            anthropicMessages.push({
                role: ai:USER,
                content: check getChatMessageStringContent(messages.content)
            });
            return anthropicMessages;
        }

        foreach ai:ChatMessage message in messages {
            if message is ai:ChatUserMessage {
                anthropicMessages.push({
                    role: ai:USER,
                    content: check getChatMessageStringContent(message.content)
                });
            } else if message is ai:ChatSystemMessage {
                // Add a user message containing the system prompt
                string content = check getChatMessageStringContent(message.content);
                anthropicMessages.push({
                    role: ai:USER,
                    content: string `<system>${content}</system>\n\n`
                });
            } else if message is ai:ChatAssistantMessage && message.content is string {
                anthropicMessages.push({
                    role: ai:ASSISTANT,
                    content: message.content ?: ""
                });
            } else if message is ai:ChatFunctionMessage && message.content is string {
                // Include function results as user messages with special formatting
                anthropicMessages.push({
                    role: ai:USER,
                    content: string `<function_results>\nFunction: ${message.name}\n`
                        + string `Output: ${message.content ?: ""}\n</function_results>`
                });
            }
        }
        return anthropicMessages;
    }

    # Maps ai:ChatCompletionFunctions to Anthropic's tool format
    #
    # + tools - Array of tool definitions
    # + return - Array of Anthropic tool definitions
    private isolated function mapToAnthropicTools(ai:ChatCompletionFunctions[] tools) returns AnthropicTool[] {
        AnthropicTool[] anthropicTools = [];

        foreach ai:ChatCompletionFunctions tool in tools {
            map<json> schema = tool.parameters ?: {'type: "object", properties: {}};

            // Create Anthropic tool with input_schema instead of parameters
            AnthropicTool AnthropicTool = {
                name: tool.name,
                description: tool.description,
                input_schema: schema
            };

            anthropicTools.push(AnthropicTool);
        }

        return anthropicTools;
    }
}

isolated function mapContentToFunctionCall(ContentBlock block) returns ai:FunctionCall|ai:LlmError {
    string? blockName = block.name;
    if blockName is () {
        return error ai:LlmError("Invalid or malformed name received in function call response.");
    }
    json inputJson = block?.input;
    map<json>?|error arguments = inputJson.cloneWithType();
    if arguments is error {
        return error ai:LlmError("Invalid or malformed arguments received in function call response.", arguments);
    }
    return {name: blockName, arguments, id: block?.id};
}

isolated function getChatMessageStringContent(ai:Prompt|string prompt) returns string|ai:Error {
    if prompt is string {
        return prompt;
    }
    string[] & readonly strings = prompt.strings;
    anydata[] insertions = prompt.insertions;
    string promptStr = strings[0];
    foreach int i in 0 ..< insertions.length() {
        string str = strings[i + 1];
        anydata insertion = insertions[i];

        if insertion is ai:TextDocument|ai:TextChunk {
            promptStr += insertion.content + " " + str;
            continue;
        }

        if insertion is ai:TextDocument[] {
            foreach ai:TextDocument doc in insertion {
                promptStr += doc.content + " ";
            }
            promptStr += str;
            continue;
        }

        if insertion is ai:TextChunk[] {
            foreach ai:TextChunk doc in insertion {
                promptStr += doc.content + " ";
            }
            promptStr += str;
            continue;
        }

        if insertion is ai:Document {
            return error ai:Error("Only Text Documents are currently supported.");
        }

        promptStr += insertion.toString() + str;
    }
    return promptStr.trim();
}
